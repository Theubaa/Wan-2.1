# ğŸŒŒ Wan 2.1 â€” Text-to-Video AI Generator

> Generate videos straight from natural language prompts using Alibabaâ€™s cutting-edge **Wan 2.1 T2V 1.3B** model.

[![License](https://img.shields.io/badge/license-educational-blue.svg)](#license)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](#)
[![Torch](https://img.shields.io/badge/powered_by-pytorch-red.svg)](#)

---

## âœ¨ Why This Repo?

This project was built to **explore the future of AI-driven media** using open-source tools. With **Wan 2.1**, you can generate videos from just a text description â€” unlocking potential in:

- ğŸ¨ Creative storytelling  
- ğŸ§ª Research workflows  
- ğŸï¸ Automated content generation

> ğŸ–¥ï¸ Runs even on modest hardware like **RTX 3050 + i5 13th Gen + Windows 11**  
> ğŸ”‹ Supports offloading and memory-efficient execution

---

## ğŸš€ Features

âœ… Text-to-Video Generation  
âœ… Pretrained 1.3B model (UMT5 + VAE)  
âœ… Offloading for low VRAM GPUs  
âœ… Torch-based, modular architecture  
âœ… Windows/Linux friendly setup  

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/Theubaa/Wan-2.1.git
cd Wan-2.1


2ï¸âƒ£ (Optional) Set up a virtual environment
bash

venv\Scripts\activate  # On Windows
3ï¸âƒ£ Install required packages

pip install torch torchvision einops transformers ftfy accelerate easydict
ğŸ¬ Quick Start

python generate.py `
  --task t2v-1.3B `
  --size 832*480 `
  --ckpt_dir ./Wan2.1-T2V-1.3B `
  --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage." `
  --offload_model True `
  --t5_cpu `
  --sample_shift 8 `
  --sample_guide_scale 6
ğŸ’¡ PowerShell users: Use backticks (`) for line breaks
ğŸ’¡ Bash users: Use \ instead

ğŸ“ Checkpoint Files
Create a folder named Wan2.1-T2V-1.3B and place the following inside:

models_t5_umt5-xxl-enc-bf16.pth

Wan2.1_VAE.pth

ğŸ” These files are not included in this repo. You must obtain them directly from the authors or release site.

âš™ï¸ Optional Flags
Flag	Description
--sample_steps	Number of denoising steps (default: 50)
--sample_shift	Time shift for style diversity
--sample_guide_scale	Guidance strength for prompt adherence

âš ï¸ Notes
ğŸ•’ Generation may take 2â€“5 minutes depending on GPU

ğŸ§  Use --t5_cpu + --offload_model to reduce VRAM use

ğŸ”’ FlashAttention fallback is handled automatically

ğŸ“„ License
This repository is provided for educational and experimental use only.
All model weights, research, and architecture are the property of Alibaba DAMO Academy.

ğŸ™Œ Acknowledgements
ğŸ¤– Alibaba Wan Team

ğŸ”¥ HuggingFace Transformers

ğŸ§  PyTorch, Accelerate, TorchVision, Einops

Made with â¤ï¸ by Theubaa
